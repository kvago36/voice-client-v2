import { useState, useRef, useEffect } from "react";

import { WS_HOST } from "../constants";

import { Pointers, WasmExports, ProcessAudioFn } from "../types";

interface VoiceRecorderProps {
  onSave: (message: string) => void;
}

const SAMPLES_COUNT = 16384;

export default function VoiceRecorder({ onSave }: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [isReady, setIsReady] = useState(false);
  const [message, setMessage] = useState("");
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const mediaSourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const audioCtxRef = useRef<AudioContext | null>(null);
  const AudioNodeRef = useRef<AudioWorkletNode | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const bufferRef = useRef<number[]>([]);
  const memoryRef = useRef<WebAssembly.Memory>(null);
  const pointerRef = useRef<Pointers>(null);
  const processAudio = useRef<ProcessAudioFn | null>(null);

  useEffect(() => {
    const ws = new WebSocket(WS_HOST);

    const memory = new WebAssembly.Memory({
      initial: 32,
      maximum: 64,
      shared: true,
    });

    const importObject = {
      env: {
        memory,
        console_log: (arg: string) => {
          console.log(arg);
        },
      },
    };

    WebAssembly.instantiateStreaming(
      fetch("native_webaudio_rust.wasm"),
      importObject
    ).then((module) => {
      const exports = module.instance.exports as unknown as WasmExports;

      const { alloc_f32, alloc_i16, process_audio_simd } = exports;

      const input_ptr_1 = alloc_f32(SAMPLES_COUNT);
      const input_ptr_2 = alloc_f32(SAMPLES_COUNT);
      const output_ptr = alloc_i16(SAMPLES_COUNT);

      processAudio.current = process_audio_simd;
      memoryRef.current = memory;
      pointerRef.current = {
        input_ptr_1,
        input_ptr_2,
        output_ptr,
      };
    });

    ws.onopen = () => {
      console.log("WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω");
      setIsConnected(true);
    };

    ws.onmessage = (event) => {
      console.log("–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ:", event.data);
      setMessage(event.data);
    };

    ws.onclose = () => {
      console.log("WebSocket –æ—Ç–∫–ª—é—á–µ–Ω");
      setIsConnected(false);
    };

    ws.onerror = (error) => {
      console.error("WebSocket –æ—à–∏–±–∫–∞:", error);
    };

    wsRef.current = ws;

    return () => {
      ws.close();
    };
  }, []);

  const handleSave = () => {
    onSave(message);
    setMessage("");
  };

  const startRecording = async () => {
    setMessage("");

    setIsReady(false);
    bufferRef.current = [];

    try {
      audioCtxRef.current = new AudioContext();
      await audioCtxRef.current.audioWorklet.addModule("processor.js");
      mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({
        audio: true,
      });
      mediaSourceRef.current = audioCtxRef.current.createMediaStreamSource(
        mediaStreamRef.current
      );

      AudioNodeRef.current = new AudioWorkletNode(
        audioCtxRef.current,
        "mic-processor",
        {
          processorOptions: {
            main_input: pointerRef.current!.input_ptr_1,
            secondary_input: pointerRef.current!.input_ptr_2,
            buffer: memoryRef.current!.buffer,
            len: SAMPLES_COUNT,
          },
        }
      );

      AudioNodeRef.current.port.onmessage = (e) => {
        const process_audio_simd = processAudio.current!;

        process_audio_simd(
          e.data.ready
            ? pointerRef.current!.input_ptr_2
            : pointerRef.current!.input_ptr_1,
          pointerRef.current!.output_ptr,
          SAMPLES_COUNT
        );

        const copy = new Int16Array(
          memoryRef.current!.buffer,
          pointerRef.current!.output_ptr,
          SAMPLES_COUNT
        );

        bufferRef.current.push(...copy);

        wsRef.current?.send(new Int16Array(copy).buffer);
      };

      // –°–æ–µ–¥–∏–Ω—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ —Å –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–º
      mediaSourceRef.current
        .connect(AudioNodeRef.current)
        .connect(audioCtxRef.current.destination);

      setIsRecording(true);
    } catch (error) {
      console.error("–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É:", error);
    }
  };

  const stopRecording = () => {
    wsRef.current?.send("end");

    // 1. –û—Ç–∫–ª—é—á–∞–µ–º worklet
    if (AudioNodeRef.current) {
      console.info("–û—Ç–∫–ª—é—á–∞–µ–º worklet");
      AudioNodeRef.current.disconnect();
      AudioNodeRef.current = null;
    }

    // 2. –û—Ç–∫–ª—é—á–∞–µ–º source
    if (mediaSourceRef.current) {
      console.info("–û—Ç–∫–ª—é—á–∞–µ–º source");
      mediaSourceRef.current.disconnect();
      mediaSourceRef.current = null;
    }

    // 3. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ–¥–∏–∞-–ø–æ—Ç–æ–∫
    if (mediaStreamRef.current) {
      console.info("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ–¥–∏–∞-–ø–æ—Ç–æ–∫");
      mediaStreamRef.current.getTracks().forEach((track) => {
        track.stop();
      });
      mediaStreamRef.current = null;
    }

    // 4. –ó–∞–∫—Ä—ã–≤–∞–µ–º AudioContext
    if (audioCtxRef.current && audioCtxRef.current.state !== "closed") {
      console.info("–ó–∞–∫—Ä—ã–≤–∞–µ–º AudioContext");
      audioCtxRef.current.close();
      audioCtxRef.current = null;
    }

    setIsReady(true);
    setIsRecording(false);

    console.log("–ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞");
  };

  const testRecord = async () => {
    const int16 = bufferRef.current;
    const audioContext = new AudioContext();

    // –í–∞–∂–Ω–æ! –†–∞–∑—Ä–µ—à–∏—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
    if (audioContext.state === "suspended") {
      await audioContext.resume();
    }

    // –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ Float32
    const float32Array = new Float32Array(int16.length);
    for (let i = 0; i < int16.length; i++) {
      float32Array[i] = int16[i] / 32768;
    }

    const audioBuffer = audioContext.createBuffer(
      1,
      float32Array.length,
      44100
    );
    audioBuffer.copyToChannel(float32Array, 0);

    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(audioContext.destination);
    source.start();

    console.log("–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º:", int16.length, "—Å—ç–º–ø–ª–æ–≤");
  };

  return (
    <>
      <button
        disabled={!isConnected}
        className={`my-4 py-2 text-white rounded cursor-pointer disabled:bg-gray-400 disabled:cursor-not-allowed ${
          isRecording ? "bg-red-500" : "bg-green-500"
        } ${isRecording ? "px-6" : "pl-2 pr-6"}`}
        onClick={isRecording ? stopRecording : startRecording}
      >
        {isRecording ? "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å" : "üéô –ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"}
      </button>

      <button
        disabled={!isReady}
        className={`my-4 ml-2 pl-2 pr-6 py-2 text-white rounded bg-purple-400 disabled:bg-gray-400 disabled:cursor-not-allowed`}
        onClick={testRecord}
      >
        üîä –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏
      </button>

      <div className="flex items-center gap-4 p-3 my-3 shadow bg-white">
        <span className="text-gray-700 font-medium">–í–∞—à –∑–∞–ø—Ä–æ—Å:</span>
        {message && (
          <>
            <span className="font-semibold">{message}</span>
            <div className="flex-row">
              {!isRecording && (
                <>
                  <div className="flex">
                    <button
                      onClick={handleSave}
                      className="flex items-center gap-1 p-2 text-white rounded-lg"
                    >
                      ‚úÖ
                    </button>
                    <button
                      onClick={() => setMessage("")}
                      className="flex items-center gap-1 p-2 text-white rounded-lg"
                    >
                      ‚ùå
                    </button>
                  </div>
                </>
              )}
            </div>
          </>
        )}
      </div>
    </>
  );
}
